{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read orginal books to strings\n",
    "f = open(\"Dick Philip. Ubik.txt\",\"r\")\n",
    "ubik = f.read()\n",
    "f = open(\"Dick Philip. Valis.txt\",\"r\")\n",
    "valis = f.read()\n",
    "f = open(\"Dick Philip. A Maze of Death.txt\",\"r\")\n",
    "maze = f.read()\n",
    "f = open(\"Dick Philip. Radio Free Albemuth.txt\",\"r\")\n",
    "radio = f.read()\n",
    "f = open(\"Dick Philip. The Three Stigmata of Palmer Eldritch.txt\",\"r\")\n",
    "palmer = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create corpus from books\n",
    "# Concatuate in one list\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def books_to_corpus(books):\n",
    "    corpus = list()\n",
    "    for i in books:\n",
    "        # Tokenize the article: tokens\n",
    "        tokens = word_tokenize(i)\n",
    "        # Convert the tokens into lowercase: lower_tokens\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        # Retain alphabetic words: alpha_only\n",
    "        alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "        # Remove all stop words: no_stops\n",
    "        no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "        # Instantiate the WordNetLemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # Lemmatize all tokens into a new list: lemmatized\n",
    "        lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "        corpus.append(lemmatized) \n",
    "    return corpus\n",
    "\n",
    "titles = ['ubik', 'valis', 'maze', 'radio', 'palmer']\n",
    "corpus_dick = books_to_corpus([ubik, valis, maze, radio, palmer])\n",
    "\n",
    "dictofdick = dict(zip(titles, corpus_dick))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dick Philip. Ubik 50 top words \n",
      "\n",
      "[('said', 1184), ('joe', 717), ('runciter', 467), ('one', 239), ('al', 204), ('know', 191), ('back', 181), ('u', 179), ('get', 163), ('time', 161), ('could', 160), ('like', 160), ('pat', 157), ('would', 156), ('thought', 149), ('chip', 132), ('asked', 130), ('way', 110), ('ubik', 106), ('right', 104), ('door', 104), ('room', 103), ('go', 103), ('see', 102), ('think', 96), ('denny', 96), ('made', 91), ('hand', 90), ('tell', 90), ('want', 90), ('moratorium', 90), ('going', 89), ('maybe', 88), ('ca', 86), ('make', 85), ('say', 84), ('jory', 84), ('new', 81), ('voice', 81), ('come', 81), ('got', 80), ('take', 80), ('ella', 77), ('face', 76), ('look', 75), ('two', 74), ('something', 74), ('hollis', 69), ('year', 64), ('girl', 64)] \n",
      "\n",
      "Dick Philip. Valis 50 top words \n",
      "\n",
      "[('said', 1109), ('fat', 979), ('kevin', 365), ('time', 307), ('one', 302), ('god', 288), ('could', 251), ('u', 243), ('would', 242), ('sherri', 194), ('know', 187), ('like', 157), ('say', 155), ('see', 138), ('back', 134), ('world', 133), ('two', 131), ('vali', 128), ('information', 128), ('life', 126), ('never', 120), ('mind', 120), ('way', 119), ('people', 119), ('go', 113), ('get', 109), ('day', 106), ('david', 106), ('linda', 104), ('gloria', 101), ('lampton', 100), ('universe', 96), ('thing', 95), ('form', 94), ('mini', 94), ('knew', 93), ('something', 90), ('thought', 88), ('going', 88), ('death', 86), ('man', 85), ('eric', 85), ('year', 81), ('sophia', 81), ('got', 80), ('right', 80), ('mean', 80), ('well', 79), ('cat', 79), ('nothing', 78)] \n",
      "\n",
      "Dick Philip. A Maze of Death 50 top words \n",
      "\n",
      "[('said', 1136), ('morley', 454), ('seth', 301), ('one', 234), ('belsnor', 221), ('know', 183), ('babble', 180), ('thought', 165), ('russell', 153), ('could', 150), ('like', 149), ('get', 148), ('u', 143), ('back', 142), ('would', 141), ('time', 135), ('see', 117), ('think', 113), ('thugg', 113), ('frazer', 108), ('maggie', 106), ('asked', 99), ('go', 98), ('want', 97), ('mary', 97), ('man', 90), ('maybe', 85), ('way', 85), ('say', 84), ('come', 84), ('right', 83), ('hand', 82), ('walsh', 79), ('saw', 79), ('susie', 79), ('away', 78), ('tallchief', 76), ('building', 73), ('made', 70), ('toward', 70), ('squib', 70), ('something', 66), ('felt', 66), ('never', 66), ('form', 65), ('ship', 64), ('last', 64), ('nothing', 64), ('thing', 63), ('yes', 63)] \n",
      "\n",
      "Dick Philip. Radio Free Albemuth 50 top words \n",
      "\n",
      "[('said', 1079), ('nicholas', 427), ('would', 306), ('could', 305), ('one', 275), ('time', 237), ('know', 233), ('like', 194), ('u', 186), ('sadassa', 139), ('fremont', 134), ('rachel', 131), ('see', 130), ('never', 126), ('way', 124), ('back', 122), ('life', 117), ('people', 116), ('vivian', 116), ('record', 115), ('thought', 113), ('get', 112), ('asked', 111), ('vali', 108), ('told', 106), ('two', 105), ('going', 105), ('knew', 104), ('aramchek', 104), ('think', 101), ('voice', 100), ('berkeley', 100), ('well', 100), ('satellite', 100), ('phil', 99), ('world', 98), ('go', 98), ('tell', 96), ('got', 93), ('say', 93), ('come', 92), ('right', 91), ('something', 91), ('still', 90), ('day', 89), ('friend', 88), ('nothing', 88), ('saw', 86), ('want', 85), ('good', 84)] \n",
      "\n",
      "Dick Philip. The Three Stigmata of Palmer Eldritch 50 top words \n",
      "\n",
      "[('said', 1019), ('leo', 397), ('barney', 394), ('eldritch', 335), ('one', 242), ('back', 224), ('know', 209), ('like', 201), ('would', 185), ('palmer', 184), ('could', 171), ('get', 170), ('time', 166), ('thought', 166), ('even', 160), ('mayerson', 153), ('going', 136), ('go', 129), ('see', 126), ('want', 121), ('got', 120), ('right', 114), ('way', 114), ('bulero', 109), ('something', 99), ('anne', 99), ('emily', 98), ('layout', 93), ('maybe', 90), ('un', 89), ('u', 89), ('make', 87), ('mean', 86), ('mar', 85), ('nothing', 83), ('hand', 83), ('eye', 82), ('new', 81), ('still', 81), ('man', 81), ('hnatt', 81), ('say', 80), ('ship', 80), ('come', 78), ('made', 77), ('think', 77), ('life', 76), ('miss', 75), ('fugate', 74), ('roni', 74)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def bow(text, top):\n",
    "    # Create the bag-of-words: bow\n",
    "    bow = Counter(text)\n",
    "    # Print most common tokens\n",
    "    return bow.most_common(top)\n",
    "\n",
    "print('Dick Philip. Ubik 50 top words \\n')\n",
    "print(bow(dictofdick['ubik'], 50), '\\n')\n",
    "print('Dick Philip. Valis 50 top words \\n')\n",
    "print(bow(dictofdick['valis'], 50), '\\n')\n",
    "print('Dick Philip. A Maze of Death 50 top words \\n')\n",
    "print(bow(dictofdick['maze'], 50), '\\n')\n",
    "print('Dick Philip. Radio Free Albemuth 50 top words \\n')\n",
    "print(bow(dictofdick['radio'], 50), '\\n')\n",
    "print('Dick Philip. The Three Stigmata of Palmer Eldritch 50 top words \\n')\n",
    "print(bow(dictofdick['palmer'], 50), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 1), (2, 8), (3, 1), (4, 1), (5, 1), (6, 16), (7, 28), (8, 1), (9, 6)]\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(list(dictofdick.values()))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "dick_corpus = [dictionary.doc2bow(article) for article in list(dictofdick.values())]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the first document, Ubik\n",
    "print(dick_corpus[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dick Philip. Ubik 50 top frequent words using Gensim without tfidf \n",
      "\n",
      "said 5527\n",
      "one 1292\n",
      "could 1037\n",
      "would 1030\n",
      "time 1006\n",
      "know 1003\n",
      "fat 992\n",
      "like 861\n",
      "u 840\n",
      "back 803\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Save the fifth document, Ubik\n",
    "ubik_gensim = dick_corpus[0]\n",
    "\n",
    "def print_words_freq(book, corpus, dictionary, top):\n",
    "    # Sort the doc for frequency: bow_doc\n",
    "    bow_doc = sorted(book, key=lambda w: w[1], reverse=True)\n",
    "    # Print the top 5 words of the document alongside the count\n",
    "#     for word_id, word_count in bow_doc[:5]:\n",
    "#         print(dictionary.get(word_id), word_count)\n",
    "    # Create the defaultdict: total_word_count\n",
    "    total_word_count = defaultdict(int)\n",
    "    for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "        total_word_count[word_id] += word_count\n",
    "    # Create a sorted list from the defaultdict: sorted_word_count\n",
    "    sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "    # Print the top 5 words across all documents alongside the count\n",
    "    for word_id, word_count in sorted_word_count[:top]:\n",
    "        print(dictionary.get(word_id), word_count)\n",
    "        \n",
    "print('Dick Philip. Ubik 50 top frequent words using Gensim without tfidf \\n')\n",
    "print_words_freq(ubik_gensim, dick_corpus, dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dick Philip. Ubik 15 top frequent words with tfidf \n",
      "\n",
      "runciter 0.683\n",
      "joe 0.597\n",
      "denny 0.1404\n",
      "moratorium 0.1316\n",
      "jory 0.1228\n",
      "hollis 0.1009\n",
      "wendy 0.0892\n",
      "ubik 0.0883\n",
      "inertials 0.079\n",
      "hammond 0.076\n",
      "vogelsang 0.0673\n",
      "ella 0.0641\n",
      "ashwood 0.0629\n",
      "chip 0.0613\n",
      "conley 0.06\n",
      "\n",
      "Dick Philip. Valis 15 top frequent words with tfidf\n",
      "\n",
      "kevin 0.7057\n",
      "sherri 0.3751\n",
      "gloria 0.1953\n",
      "lampton 0.1933\n",
      "mini 0.1817\n",
      "sophia 0.1566\n",
      "vali 0.1409\n",
      "horselover 0.1353\n",
      "david 0.1167\n",
      "linda 0.1145\n",
      "zebra 0.1025\n",
      "eric 0.0936\n",
      "maurice 0.0909\n",
      "fremount 0.0889\n",
      "plasmate 0.0812\n",
      "\n",
      "Dick Philip. A Maze of Death 15 top frequent words with tfidf\n",
      "\n",
      "morley 0.7029\n",
      "belsnor 0.3422\n",
      "babble 0.2787\n",
      "seth 0.2653\n",
      "thugg 0.175\n",
      "frazer 0.1672\n",
      "maggie 0.1641\n",
      "russell 0.1349\n",
      "susie 0.1223\n",
      "walsh 0.1223\n",
      "tallchief 0.1177\n",
      "squib 0.1084\n",
      "wade 0.0929\n",
      "noser 0.0774\n",
      "betty 0.0681\n",
      "\n",
      "Dick Philip. Radio Free Albemuth 15 top frequent words with tfidf\n",
      "\n",
      "nicholas 0.6086\n",
      "fremont 0.3354\n",
      "rachel 0.3279\n",
      "vivian 0.2904\n",
      "aramchek 0.2603\n",
      "sadassa 0.1981\n",
      "vali 0.1539\n",
      "phil 0.1411\n",
      "kaplan 0.1126\n",
      "progressive 0.1101\n",
      "silvia 0.1076\n",
      "fap 0.1001\n",
      "brady 0.0912\n",
      "nick 0.0901\n",
      "berkeley 0.0795\n",
      "\n",
      "Dick Philip. The Three Stigmata of Palmer Eldritch 15 top frequent words with tfidf\n",
      "\n",
      "leo 0.5908\n",
      "barney 0.5864\n",
      "mayerson 0.2277\n",
      "bulero 0.1622\n",
      "eldritch 0.1582\n",
      "anne 0.1473\n",
      "emily 0.1459\n",
      "un 0.1325\n",
      "hnatt 0.1206\n",
      "fugate 0.1101\n",
      "hovel 0.1101\n",
      "roni 0.1101\n",
      "schein 0.0908\n",
      "palmer 0.0869\n",
      "layout 0.0788\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfModel\n",
    "from gensim.models.tfidfmodel import TfidfModel \n",
    "\n",
    "def print_tfidf_freq(book, corpus, dictionary, top = 10, order = True):\n",
    "\n",
    "    # Create a new TfidfModel using the corpus: tfidf\n",
    "    tfidf = TfidfModel(corpus)\n",
    "\n",
    "    # Calculate the tfidf weights of doc: tfidf_weights\n",
    "    tfidf_weights = tfidf[book]\n",
    "\n",
    "    # # Print the first five weights\n",
    "    # print(tfidf_weights[:5])\n",
    "\n",
    "    # Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=order)\n",
    "    \n",
    "   \n",
    "    # Print the top 5 weighted words\n",
    "    for term_id, weight in sorted_tfidf_weights[:top]:\n",
    "        print(dictionary.get(term_id), round(weight,4))\n",
    "\n",
    "print('Dick Philip. Ubik 15 top frequent words with tfidf \\n')\n",
    "print_tfidf_freq(dick_corpus[0], dick_corpus, dictionary, 15)\n",
    "print('\\nDick Philip. Valis 15 top frequent words with tfidf\\n')\n",
    "print_tfidf_freq(dick_corpus[1], dick_corpus, dictionary, 15)\n",
    "print('\\nDick Philip. A Maze of Death 15 top frequent words with tfidf\\n')\n",
    "print_tfidf_freq(dick_corpus[2], dick_corpus, dictionary, 15)\n",
    "print('\\nDick Philip. Radio Free Albemuth 15 top frequent words with tfidf\\n')\n",
    "print_tfidf_freq(dick_corpus[3], dick_corpus, dictionary, 15)\n",
    "print('\\nDick Philip. The Three Stigmata of Palmer Eldritch 15 top frequent words with tfidf\\n')\n",
    "print_tfidf_freq(dick_corpus[4], dick_corpus, dictionary, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dick Philip. Ubik 15 top infrequent words with tdidf \n",
      "\n",
      "abstract 0.0002\n",
      "admittedly 0.0002\n",
      "affair 0.0002\n",
      "affect 0.0002\n",
      "aimed 0.0002\n",
      "alertly 0.0002\n",
      "allow 0.0002\n",
      "america 0.0002\n",
      "amuse 0.0002\n",
      "amused 0.0002\n",
      "amusement 0.0002\n",
      "angrily 0.0002\n",
      "animosity 0.0002\n",
      "apartment 0.0002\n",
      "appeal 0.0002\n",
      "\n",
      "Dick Philip. Valis 15 top infrequent words with tdidf\n",
      "\n",
      "accustomed 0.0003\n",
      "admittedly 0.0003\n",
      "afford 0.0003\n",
      "al 0.0003\n",
      "alertly 0.0003\n",
      "amuse 0.0003\n",
      "amusement 0.0003\n",
      "animosity 0.0003\n",
      "appeal 0.0003\n",
      "arrest 0.0003\n",
      "aside 0.0003\n",
      "assassin 0.0003\n",
      "attorney 0.0003\n",
      "bait 0.0003\n",
      "beamed 0.0003\n",
      "\n",
      "Dick Philip. A Maze of Death 15 top infrequent words with tdidf\n",
      "\n",
      "abstract 0.0002\n",
      "aching 0.0002\n",
      "admire 0.0002\n",
      "aid 0.0002\n",
      "aimed 0.0002\n",
      "alertly 0.0002\n",
      "america 0.0002\n",
      "amuse 0.0002\n",
      "amused 0.0002\n",
      "appalled 0.0002\n",
      "artifact 0.0002\n",
      "ascended 0.0002\n",
      "augmented 0.0002\n",
      "avoid 0.0002\n",
      "awaken 0.0002\n",
      "\n",
      "Dick Philip. Radio Free Albemuth 15 top infrequent words with tdidf\n",
      "\n",
      "accustomed 0.0003\n",
      "aching 0.0003\n",
      "adequate 0.0003\n",
      "admire 0.0003\n",
      "afford 0.0003\n",
      "al 0.0003\n",
      "animosity 0.0003\n",
      "approval 0.0003\n",
      "arrival 0.0003\n",
      "ascended 0.0003\n",
      "assassin 0.0003\n",
      "augmented 0.0003\n",
      "bait 0.0003\n",
      "bear 0.0003\n",
      "belt 0.0003\n",
      "\n",
      "Dick Philip. The Three Stigmata of Palmer Eldritch 15 top infrequent words with tdidf\n",
      "\n",
      "admire 0.0002\n",
      "aimed 0.0002\n",
      "al 0.0002\n",
      "alleged 0.0002\n",
      "amuse 0.0002\n",
      "amusement 0.0002\n",
      "animosity 0.0002\n",
      "apartment 0.0002\n",
      "appalled 0.0002\n",
      "appointment 0.0002\n",
      "arrest 0.0002\n",
      "ascended 0.0002\n",
      "assassin 0.0002\n",
      "attorney 0.0002\n",
      "augmented 0.0002\n"
     ]
    }
   ],
   "source": [
    "print('Dick Philip. Ubik 15 top infrequent words with tdidf \\n')\n",
    "print_tfidf_freq(dick_corpus[0], dick_corpus, dictionary, 15, False)\n",
    "print('\\nDick Philip. Valis 15 top infrequent words with tdidf\\n')\n",
    "print_tfidf_freq(dick_corpus[1], dick_corpus, dictionary, 15, False)\n",
    "print('\\nDick Philip. A Maze of Death 15 top infrequent words with tdidf\\n')\n",
    "print_tfidf_freq(dick_corpus[2], dick_corpus, dictionary, 15, False)\n",
    "print('\\nDick Philip. Radio Free Albemuth 15 top infrequent words with tdidf\\n')\n",
    "print_tfidf_freq(dick_corpus[3], dick_corpus, dictionary, 15, False)\n",
    "print('\\nDick Philip. The Three Stigmata of Palmer Eldritch 15 top infrequent words with tdidf\\n')\n",
    "print_tfidf_freq(dick_corpus[4], dick_corpus, dictionary, 15, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Named entity recognition\n",
    "def ner(book, part):\n",
    "\n",
    "    # Tokenize the article into sentences: sentences\n",
    "    sentences = sent_tokenize(book)\n",
    "\n",
    "    # Tokenize each sentence into words: token_sentences\n",
    "    token_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "    # Tag each tokenized sentence into parts of speech: pos_sentences\n",
    "    pos_sentences = [pos_tag(sent) for sent in token_sentences] \n",
    "\n",
    "    # Create the named entity chunks: chunked_sentences\n",
    "    chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
    "    \n",
    "    result = dict()\n",
    "    # Test for stems of the tree with 'part' tags\n",
    "    for sent in chunked_sentences:\n",
    "        for chunk in sent:\n",
    "            if hasattr(chunk, \"label\") and chunk.label() == part:\n",
    "                result[str(chunk)] = result.get(str(chunk), 0) + 1\n",
    "    return result            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 proper nouns from Ubik: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('(NE Joe/NNP)', 617),\n",
       " ('(NE Runciter/NNP)', 205),\n",
       " ('(NE Al/NNP)', 115),\n",
       " ('(NE Pat/NNP)', 101),\n",
       " ('(NE Joe/NNP Chip/NNP)', 72),\n",
       " ('(NE Ubik/NNP)', 67),\n",
       " ('(NE Denny/NNP)', 52),\n",
       " ('(NE Ella/NNP)', 50),\n",
       " ('(NE Jory/NNP)', 45),\n",
       " ('(NE Hollis/NNP)', 40),\n",
       " ('(NE Glen/NNP Runciter/NNP)', 38),\n",
       " ('(NE New/NNP York/NNP)', 38),\n",
       " ('(NE Wendy/NNP)', 36),\n",
       " ('(NE Mr./NNP Runciter/NNP)', 35),\n",
       " ('(NE Don/NNP Denny/NNP)', 35)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ubik_ner_dict = ner(ubik, 'NE')\n",
    "ubik_ner_list = [(w, ubik_ner_dict[w]) for w in sorted(ubik_ner_dict, key=ubik_ner_dict.get, reverse=True)]\n",
    "print('Top 15 proper nouns from Ubik: \\n')\n",
    "ubik_ner_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 proper nouns from Valis: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('(NE Fat/NNP)', 816),\n",
       " ('(NE Kevin/NNP)', 343),\n",
       " ('(NE Sherri/NNP)', 173),\n",
       " ('(NE David/NNP)', 106),\n",
       " ('(NE Gloria/NNP)', 91),\n",
       " ('(NE Mini/NNP)', 75),\n",
       " ('(NE VALIS/NNP)', 69),\n",
       " ('(NE Sophia/NNP)', 63),\n",
       " ('(NE Linda/NNP)', 59),\n",
       " ('(NE God/NNP)', 56),\n",
       " ('(NE Horselover/NNP Fat/NNP)', 56),\n",
       " ('(NE Empire/NNP)', 55),\n",
       " ('(NE Savior/NNP)', 52),\n",
       " ('(NE Christ/NNP)', 47),\n",
       " ('(NE Zebra/NNP)', 40)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valis_ner_dict = ner(valis, 'NE')\n",
    "valis_ner_list = [(w, valis_ner_dict[w]) for w in sorted(valis_ner_dict, key=valis_ner_dict.get, reverse=True)]\n",
    "print('Top 15 proper nouns from Valis: \\n')\n",
    "valis_ner_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 proper nouns from A Maze of Death: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('(NE Nicholas/NNP)', 349),\n",
       " ('(NE Rachel/NNP)', 126),\n",
       " ('(NE Sadassa/NNP)', 97),\n",
       " ('(NE Berkeley/NNP)', 94),\n",
       " ('(NE Phil/NNP)', 84),\n",
       " ('(NE Aramchek/NNP)', 66),\n",
       " ('(NE Valis/NNP)', 66),\n",
       " ('(NE Ferris/NNP Fremont/NNP)', 57),\n",
       " ('(NE Fremont/NNP)', 51),\n",
       " ('(NE Vivian/NNP)', 44),\n",
       " ('(NE Johnny/NNP)', 37),\n",
       " ('(NE Party/NNP)', 31),\n",
       " ('(NE Orange/NNP County/NNP)', 31),\n",
       " ('(NE Leon/NNP)', 30),\n",
       " ('(NE FAP/NNP)', 29)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radio_ner_dict = ner(radio, 'NE')\n",
    "radio_ner_list = [(w, radio_ner_dict[w]) for w in sorted(radio_ner_dict, key=radio_ner_dict.get, reverse=True)]\n",
    "print('Top 15 proper nouns from A Maze of Death: \\n')\n",
    "radio_ner_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 proper nouns from Radio Free Albemuth: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('(NE Seth/NNP Morley/NNP)', 201),\n",
       " ('(NE Russell/NNP)', 118),\n",
       " ('(NE Babble/NNP)', 117),\n",
       " ('(NE Belsnor/NNP)', 95),\n",
       " ('(NE Morley/NNP)', 74),\n",
       " ('(NE Frazer/NNP)', 63),\n",
       " ('(NE Maggie/NNP Walsh/NNP)', 55),\n",
       " ('(NE Wade/NNP Frazer/NNP)', 42),\n",
       " ('(NE Form/NNP Destroyer/NNP)', 37),\n",
       " ('(NE Babble/JJ)', 37),\n",
       " ('(NE Thugg/NNP)', 31),\n",
       " ('(NE Glen/NNP Belsnor/NNP)', 30),\n",
       " ('(NE Ignatz/NNP Thugg/NNP)', 29),\n",
       " ('(NE Susie/NNP)', 28),\n",
       " ('(NE Maggie/NNP)', 28)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maze_ner_dict = ner(maze, 'NE')\n",
    "maze_ner_list = [(w, maze_ner_dict[w]) for w in sorted(maze_ner_dict, key=maze_ner_dict.get, reverse=True)]\n",
    "print('Top 15 proper nouns from Radio Free Albemuth: \\n')\n",
    "maze_ner_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 proper nouns from The Three Stigmata of Palmer Eldritch: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('(NE Barney/NNP)', 299),\n",
       " ('(NE Leo/NNP)', 166),\n",
       " ('(NE Eldritch/NNP)', 143),\n",
       " ('(NE Palmer/NNP Eldritch/NNP)', 131),\n",
       " ('(NE Anne/NNP)', 60),\n",
       " ('(NE Emily/NNP)', 58),\n",
       " ('(NE Mars/NNP)', 56),\n",
       " ('(NE Mayerson/NNP)', 48),\n",
       " ('(NE Barney/NNP Mayerson/NNP)', 47),\n",
       " ('(NE Leo/NNP Bulero/NNP)', 44),\n",
       " ('(NE Mr./NNP Mayerson/NNP)', 37),\n",
       " ('(NE Norm/NNP Schein/NNP)', 35),\n",
       " ('(NE Perky/NNP Pat/NNP)', 33),\n",
       " ('(NE Palmer/NNP)', 33),\n",
       " ('(NE Hnatt/NNP)', 32)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palmer_ner_dict = ner(palmer, 'NE')\n",
    "palmer_ner_list = [(w, palmer_ner_dict[w]) for w in sorted(palmer_ner_dict, key=palmer_ner_dict.get, reverse=True)]\n",
    "print('Top 15 proper nouns from The Three Stigmata of Palmer Eldritch: \\n')\n",
    "palmer_ner_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'(NE God/NNP)', '(NE Jesus/NNP)', '(NE None/NN)', '(NE Soviet/NNP Union/NNP)', '(NE Half/NN)', '(NE Nobody/NN)', '(NE Ferris/NNP)', '(NE Anyhow/NNP)', '(NE Okay/NNP)', '(NE German/JJ)', '(NE Christ/NN)', '(NE America/NNP)', '(NE United/NNP States/NNPS)', '(NE American/JJ)', '(NE THE/NNP)', '(NE Japan/NNP)', '(NE Germany/NNP)', '(NE Earth/NNP)', '(NE French/JJ)', '(NE San/NNP Francisco/NNP)', '(NE China/NNP)', '(NE New/NNP York/NNP)', '(NE Nothing/NN)', '(NE Christ/NNP)', '(NE Hebrew/NNP)', '(NE Someone/NN)', '(NE Please/NNP)', '(NE Philip/NNP)', '(NE No/DT)', '(NE Cleveland/NNP)', '(NE Latin/NNP)', '(NE Silence/NN)', '(NE Sorry/NNP)'}\n"
     ]
    }
   ],
   "source": [
    "# Common proper names between books\n",
    "# Comment out to check different combinations\n",
    "\n",
    "common_names = set(array(ubik_ner_list)[:,0]).intersection(\n",
    "                                                            set(array(valis_ner_list)[:,0]),\n",
    "                                                            set(array(radio_ner_list)[:,0]),\n",
    "#                                                             set(array(maze_ner_list)[:,0]),\n",
    "#                                                             set(array(palmer_ner_list)[:,0])\n",
    "                                                          )\n",
    "print(common_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('valis + maze', 148), ('ubik + palmer', 68), ('ubik + maze', 66), ('maze + palmer', 65), ('ubik + valis', 62), ('valis + palmer', 61), ('valis + radio', 56), ('radio + palmer', 51), ('ubik + radio', 49), ('maze + radio', 49)]\n"
     ]
    }
   ],
   "source": [
    "lists_names = [ubik_ner_list, valis_ner_list, radio_ner_list, maze_ner_list, palmer_ner_list]\n",
    "titles = ['ubik', 'valis', 'maze', 'radio', 'palmer']\n",
    "\n",
    "score = list()\n",
    "name = list()\n",
    "\n",
    "for i, j in list(itertools.combinations(lists_names,2)):\n",
    "    score.append((len(set(array(i)[:,0]).intersection(set(array(j)[:,0])))))       \n",
    "    \n",
    "for i, j in list(itertools.combinations(titles,2)):\n",
    "    name.append(i + ' + ' + j)      \n",
    "\n",
    "books_similiarity = zip(name,score)\n",
    "print(sorted(tuple(books_similiarity), key=lambda tup: tup[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
